{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONVERSE with REALTIME API \n",
    "In this step, we'll transform our single-turn audio generation into a full conversational system. This involves tackling several key challenges: managing bi-directional audio streams for both user input and AI output, implementing proper turn-taking mechanics (knowing when the user has finished speaking and when the AI should respond), handling the complex flow of events from speech detection to response generation, and maintaining conversation context across multiple exchanges. The trickiest part isn't just streaming the audio - it's orchestrating all these components to work together smoothly while maintaining natural conversational flow. We'll build this step by step, focusing on creating a robust system that can handle real-world conversation scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Challenges in Building Real-time Conversational AI\n",
    "\n",
    "Below are the key challanges in building Real-time Conversational AI...\n",
    "\n",
    "1. **Session and Context Management**\n",
    "  - Maintaining conversation history\n",
    "  - Tracking context across exchanges\n",
    "  - Managing session state\n",
    "\n",
    "2. **Turn-Taking Mechanics**\n",
    "  - Detecting end of user speech\n",
    "  - Managing interruptions\n",
    "  - Coordinating input/output transitions\n",
    "  - Timing audio recording and playback\n",
    "\n",
    "3. **Audio Stream Handling**\n",
    "  - Coordinating bidirectional audio\n",
    "  - Managing stream lifecycles\n",
    "  - Resolving device conflicts\n",
    "  - Handling format compatibility\n",
    "\n",
    "4. **Event Flow Control**\n",
    "  - Input audio streaming\n",
    "  - Speech detection\n",
    "  - Text transcription\n",
    "  - Response generation\n",
    "  - Output audio streaming\n",
    "\n",
    "5. **Error Recovery**\n",
    "  - Connection drops\n",
    "  - Audio device issues\n",
    "  - API rate limits\n",
    "  - Graceful error handling\n",
    "\n",
    "6. **Resource Management**\n",
    "  - WebSocket connection handling\n",
    "  - Audio buffer memory\n",
    "  - Resource cleanup\n",
    "  - Connection lifecycle management\n",
    "\n",
    "\n",
    "We will build an MVP, therefore choose the minimum set that will give us a working setup. The below is what we will target. \n",
    "# Minimum Viable Conversational System - Development Plan\n",
    "\n",
    "## Phase 1: Basic Turn-Taking\n",
    "1. **Core Components**\n",
    "  - Audio input/output stream management\n",
    "  - Basic turn detection\n",
    "  - Session context tracking\n",
    "\n",
    "## Implementation Steps\n",
    "\n",
    "### Step 1: Input/Output Setup\n",
    "- Configure audio device for both input and output\n",
    "- Implement basic audio streaming\n",
    "\n",
    "### Step 2: Turn Detection\n",
    "- Add VAD (Voice Activity Detection)\n",
    "- Implement simple silence detection\n",
    "- Basic interrupt handling\n",
    "\n",
    "### Step 3: Context Management\n",
    "- Track conversation history\n",
    "- Maintain basic session state\n",
    "\n",
    "## Testing Milestones\n",
    "1. Test audio I/O\n",
    "2. Verify turn detection\n",
    "3. Validate context preservation\n",
    "\n",
    "Timeline: We will implement each step iteratively, testing thoroughly before moving to next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 \n",
    "We will implement the below key features implemented:\n",
    "\n",
    "- Bidirectional audio streams\n",
    "- Basic turn management\n",
    "- Audio buffering\n",
    "- Session handling\n",
    "- Basic error recovery\n",
    "\n",
    "Finally test by running the script - it will listen for 2 seconds, send to API, then play response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import base64\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import websockets\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "\n",
    "class ConversationSystem:\n",
    "    def __init__(self):\n",
    "        self.input_stream = None\n",
    "        self.output_stream = None\n",
    "        self.is_speaking = False\n",
    "        self.input_buffer = []\n",
    "        self.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "        self.url = (\n",
    "            f\"wss://aoai-ep-swedencentral02.openai.azure.com/openai/realtime?\"\n",
    "            f\"api-version=2024-10-01-preview&deployment=gpt-4o-realtime-preview&api-key={self.api_key}\"\n",
    "        )\n",
    "        \n",
    "    async def setup_audio(self):\n",
    "        self.output_stream = sd.OutputStream(samplerate=24000, channels=1, dtype=np.int16)\n",
    "        self.input_stream = sd.InputStream(samplerate=24000, channels=1, dtype=np.int16,\n",
    "                                         callback=self.audio_callback)\n",
    "        self.output_stream.start()\n",
    "        self.input_stream.start()\n",
    "        \n",
    "    def audio_callback(self, indata, frames, time, status):\n",
    "        if status:\n",
    "            print(f\"Input stream error: {status}\")\n",
    "        if not self.is_speaking:\n",
    "            self.input_buffer.extend(indata.tobytes())\n",
    "            \n",
    "    async def process_turn(self):\n",
    "        try:\n",
    "            async with websockets.connect(self.url) as ws:\n",
    "                # Session setup\n",
    "                await self.setup_session(ws)\n",
    "                \n",
    "                while True:\n",
    "                    # Wait for input\n",
    "                    if len(self.input_buffer) > 48000:  # 2 seconds of audio\n",
    "                        audio_data = bytes(self.input_buffer)\n",
    "                        self.input_buffer.clear()\n",
    "                        \n",
    "                        # Send audio to API\n",
    "                        await self.send_audio(ws, audio_data)\n",
    "                        \n",
    "                        # Get and play response\n",
    "                        await self.handle_response(ws)\n",
    "                        \n",
    "                    await asyncio.sleep(0.1)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error in conversation: {e}\")\n",
    "            \n",
    "    async def setup_session(self, ws):\n",
    "        session_payload = {\n",
    "            \"type\": \"session.update\",\n",
    "            \"session\": {\n",
    "                \"voice\": \"alloy\",\n",
    "                \"instructions\": \"You are a helpful AI assistant. Keep responses brief and engaging.\",\n",
    "                \"modalities\": [\"audio\", \"text\"],\n",
    "                \"input_audio_format\": \"pcm16\",\n",
    "                \"output_audio_format\": \"pcm16\",\n",
    "                \"turn_detection\": {\n",
    "                    \"type\": \"server_vad\",\n",
    "                    \"threshold\": 0.5,\n",
    "                    \"prefix_padding_ms\": 300,\n",
    "                    \"silence_duration_ms\": 200\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        await ws.send(json.dumps(session_payload))\n",
    "        \n",
    "    async def send_audio(self, ws, audio_data):\n",
    "        # Convert and send audio data\n",
    "        base64_audio = base64.b64encode(audio_data).decode('utf-8')\n",
    "        await ws.send(json.dumps({\n",
    "            \"type\": \"input_audio_buffer.append\",\n",
    "            \"audio\": base64_audio\n",
    "        }))\n",
    "        \n",
    "    async def handle_response(self, ws):\n",
    "        self.is_speaking = True\n",
    "        try:\n",
    "            while True:\n",
    "                response = await ws.recv()\n",
    "                data = json.loads(response)\n",
    "                \n",
    "                if data[\"type\"] == \"response.audio.delta\":\n",
    "                    if \"delta\" in data:\n",
    "                        try:\n",
    "                            audio_data = data[\"delta\"].replace(\" \", \"\").replace(\"\\n\", \"\")\n",
    "                            padding = len(audio_data) % 4\n",
    "                            if padding:\n",
    "                                audio_data += \"=\" * (4 - padding)\n",
    "                            \n",
    "                            audio_bytes = base64.b64decode(audio_data)\n",
    "                            audio = np.frombuffer(audio_bytes, dtype=np.int16)\n",
    "                            self.output_stream.write(audio)\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing audio: {e}\")\n",
    "                            \n",
    "                elif data[\"type\"] == \"response.done\":\n",
    "                    break\n",
    "                    \n",
    "        finally:\n",
    "            self.is_speaking = False\n",
    "\n",
    "async def main():\n",
    "    system = ConversationSystem()\n",
    "    await system.setup_audio()\n",
    "    await system.process_turn()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the above as a python script outside of the notebook as event loops clash with Jupyters scheduling. \\\n",
    "[Here is a working script](https://github.com/ozgurgulerx/ai-builders/blob/main/part1_build/p1uc1_realtime_api_converse.py)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
