{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONVERSE with REALTIME API \n",
    "In this step, we'll transform our single-turn audio generation into a full conversational system. This involves tackling several key challenges: managing bi-directional audio streams for both user input and AI output, implementing proper turn-taking mechanics (knowing when the user has finished speaking and when the AI should respond), handling the complex flow of events from speech detection to response generation, and maintaining conversation context across multiple exchanges. The trickiest part isn't just streaming the audio - it's orchestrating all these components to work together smoothly while maintaining natural conversational flow. We'll build this step by step, focusing on creating a robust system that can handle real-world conversation scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Challenges in Building Real-time Conversational AI\n",
    "\n",
    "Below are the key challanges in building Real-time Conversational AI...\n",
    "\n",
    "1. **Session and Context Management**\n",
    "  - Maintaining conversation history\n",
    "  - Tracking context across exchanges\n",
    "  - Managing session state\n",
    "\n",
    "2. **Turn-Taking Mechanics**\n",
    "  - Detecting end of user speech\n",
    "  - Managing interruptions\n",
    "  - Coordinating input/output transitions\n",
    "  - Timing audio recording and playback\n",
    "\n",
    "3. **Audio Stream Handling**\n",
    "  - Coordinating bidirectional audio\n",
    "  - Managing stream lifecycles\n",
    "  - Resolving device conflicts\n",
    "  - Handling format compatibility\n",
    "\n",
    "4. **Event Flow Control**\n",
    "  - Input audio streaming\n",
    "  - Speech detection\n",
    "  - Text transcription\n",
    "  - Response generation\n",
    "  - Output audio streaming\n",
    "\n",
    "5. **Error Recovery**\n",
    "  - Connection drops\n",
    "  - Audio device issues\n",
    "  - API rate limits\n",
    "  - Graceful error handling\n",
    "\n",
    "6. **Resource Management**\n",
    "  - WebSocket connection handling\n",
    "  - Audio buffer memory\n",
    "  - Resource cleanup\n",
    "  - Connection lifecycle management\n",
    "\n",
    "\n",
    "We will build an MVP, therefore choose the minimum set that will give us a working setup. The below is what we will target. \n",
    "# Minimum Viable Conversational System - Development Plan\n",
    "\n",
    "## Phase 1: Basic Turn-Taking\n",
    "1. **Core Components**\n",
    "  - Audio input/output stream management\n",
    "  - Basic turn detection\n",
    "  - Session context tracking\n",
    "\n",
    "## Implementation Steps\n",
    "\n",
    "### Step 1: Input/Output Setup\n",
    "- Configure audio device for both input and output\n",
    "- Implement basic audio streaming\n",
    "\n",
    "### Step 2: Turn Detection\n",
    "- Add VAD (Voice Activity Detection)\n",
    "- Implement simple silence detection\n",
    "- Basic interrupt handling\n",
    "\n",
    "### Step 3: Context Management\n",
    "- Track conversation history\n",
    "- Maintain basic session state\n",
    "\n",
    "## Testing Milestones\n",
    "1. Test audio I/O\n",
    "2. Verify turn detection\n",
    "3. Validate context preservation\n",
    "\n",
    "Timeline: We will implement each step iteratively, testing thoroughly before moving to next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 **Session and Context Management**\n",
    "We will implement the below key features implemented:\n",
    "\n",
    "- Bidirectional audio streams\n",
    "- Basic turn management\n",
    "- Audio buffering\n",
    "- Session handling\n",
    "- Basic error recovery\n",
    "\n",
    "Finally test by running the script - it will listen for 2 seconds, send to API, then play response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import base64\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import websockets\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "\n",
    "class ConversationSystem:\n",
    "    def __init__(self):\n",
    "        load_dotenv()\n",
    "        self.input_stream = None\n",
    "        self.output_stream = None\n",
    "        self.is_speaking = False\n",
    "        self.input_buffer = []\n",
    "        self.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"AZURE_OPENAI_API_KEY not found in environment\")\n",
    "        self.url = (\n",
    "            f\"wss://aoai-ep-swedencentral02.openai.azure.com/openai/realtime?\"\n",
    "            f\"api-version=2024-10-01-preview&deployment=gpt-4o-realtime-preview&api-key={self.api_key}\"\n",
    "        )\n",
    "\n",
    "    async def setup_audio(self):\n",
    "        print(\"Setting up audio streams...\")\n",
    "        self.output_stream = sd.OutputStream(samplerate=24000, channels=1, dtype=np.int16)\n",
    "        self.input_stream = sd.InputStream(samplerate=24000, channels=1, dtype=np.int16,\n",
    "                                         callback=self.audio_callback)\n",
    "        self.output_stream.start()\n",
    "        self.input_stream.start()\n",
    "        print(\"Audio streams started\")\n",
    "        \n",
    "    def audio_callback(self, indata, frames, time, status):\n",
    "        if status:\n",
    "            print(f\"Input stream error: {status}\")\n",
    "        if not self.is_speaking:\n",
    "            self.input_buffer.extend(indata.tobytes())\n",
    "\n",
    "    async def start_conversation(self):\n",
    "        try:\n",
    "            async with websockets.connect(self.url) as ws:\n",
    "                print(\"Connected to WebSocket\")\n",
    "                await self.setup_session(ws)\n",
    "                \n",
    "                # Initial greeting\n",
    "                await self.send_message(ws, \"Hello\")\n",
    "                await self.handle_response(ws)\n",
    "\n",
    "                while True:\n",
    "                    print(\"\\nListening... (speak for at least 2 seconds)\")\n",
    "                    self.input_buffer.clear()\n",
    "                    await asyncio.sleep(2)  # Wait for 2 seconds of audio\n",
    "\n",
    "                    if len(self.input_buffer) > 0:\n",
    "                        print(\"Processing your input...\")\n",
    "                        audio_data = bytes(self.input_buffer)\n",
    "                        self.input_buffer.clear()\n",
    "                        \n",
    "                        # Send audio\n",
    "                        base64_audio = base64.b64encode(audio_data).decode('utf-8')\n",
    "                        await ws.send(json.dumps({\n",
    "                            \"type\": \"input_audio_buffer.append\",\n",
    "                            \"audio\": base64_audio\n",
    "                        }))\n",
    "                        await ws.send(json.dumps({\n",
    "                            \"type\": \"input_audio_buffer.commit\"\n",
    "                        }))\n",
    "                        \n",
    "                        # Request response\n",
    "                        await ws.send(json.dumps({\n",
    "                            \"type\": \"response.create\",\n",
    "                            \"response\": {\"modalities\": [\"audio\", \"text\"]}\n",
    "                        }))\n",
    "                        \n",
    "                        await self.handle_response(ws)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in conversation: {e}\")\n",
    "\n",
    "    async def setup_session(self, ws):\n",
    "        session_payload = {\n",
    "            \"type\": \"session.update\",\n",
    "            \"session\": {\n",
    "                \"voice\": \"alloy\",\n",
    "                \"instructions\": \"You are a helpful AI assistant. Keep responses brief and engaging.\",\n",
    "                \"modalities\": [\"audio\", \"text\"],\n",
    "                \"input_audio_format\": \"pcm16\",\n",
    "                \"output_audio_format\": \"pcm16\",\n",
    "                \"turn_detection\": {\n",
    "                    \"type\": \"server_vad\",\n",
    "                    \"threshold\": 0.5,\n",
    "                    \"prefix_padding_ms\": 300,\n",
    "                    \"silence_duration_ms\": 200\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        await ws.send(json.dumps(session_payload))\n",
    "        \n",
    "        while True:\n",
    "            response = await ws.recv()\n",
    "            data = json.loads(response)\n",
    "            if data.get(\"type\") == \"session.created\":\n",
    "                print(\"Session setup complete\")\n",
    "                break\n",
    "            elif data.get(\"type\") == \"error\":\n",
    "                raise Exception(\"Error creating session\")\n",
    "\n",
    "    async def send_message(self, ws, text):\n",
    "        message_payload = {\n",
    "            \"type\": \"conversation.item.create\",\n",
    "            \"item\": {\n",
    "                \"type\": \"message\",\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"type\": \"input_text\", \"text\": text}]\n",
    "            }\n",
    "        }\n",
    "        await ws.send(json.dumps(message_payload))\n",
    "        \n",
    "        await ws.send(json.dumps({\n",
    "            \"type\": \"response.create\",\n",
    "            \"response\": {\"modalities\": [\"audio\", \"text\"]}\n",
    "        }))\n",
    "\n",
    "    async def handle_response(self, ws):\n",
    "        self.is_speaking = True\n",
    "        try:\n",
    "            while True:\n",
    "                response = await ws.recv()\n",
    "                data = json.loads(response)\n",
    "                \n",
    "                if data[\"type\"] == \"response.audio.delta\":\n",
    "                    if \"delta\" in data:\n",
    "                        try:\n",
    "                            audio_data = data[\"delta\"].replace(\" \", \"\").replace(\"\\n\", \"\")\n",
    "                            padding = len(audio_data) % 4\n",
    "                            if padding:\n",
    "                                audio_data += \"=\" * padding\n",
    "                            \n",
    "                            audio_bytes = base64.b64decode(audio_data)\n",
    "                            audio = np.frombuffer(audio_bytes, dtype=np.int16)\n",
    "                            self.output_stream.write(audio)\n",
    "                            print(\".\", end=\"\", flush=True)\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing audio: {e}\")\n",
    "                            \n",
    "                elif data[\"type\"] == \"response.done\":\n",
    "                    break\n",
    "                    \n",
    "        finally:\n",
    "            self.is_speaking = False\n",
    "\n",
    "async def main():\n",
    "    system = ConversationSystem()\n",
    "    await system.setup_audio()\n",
    "    await system.start_conversation()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting real-time conversation system...\")\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the above as a python script outside of the notebook as event loops clash with Jupyters scheduling. \\\n",
    "[Here is a working script](https://github.com/ozgurgulerx/ai-builders/blob/main/part1_build/p1uc1_realtime_api_converse_step1.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila - our voice chatbot works!!! \\\n",
    "Still a bit clumsy though in terms of detecting session dynamics as well as properly listenning and understanding the user. \\\n",
    "We will improve it in the next steps.\n",
    "\n",
    "Implementation Status: Phase 1 \\\n",
    "\n",
    "#### âœ“ Completed\n",
    "- Audio I/O setup (device configuration, streaming)\n",
    "- Server-side VAD implementation\n",
    "- Basic audio state management\n",
    "\n",
    "#### ðŸ”„ Partially Implemented\n",
    "- Turn detection (needs refinement)\n",
    "- Basic interrupt handling\n",
    "- State tracking\n",
    "\n",
    "##### âŒ Not Implemented\n",
    "- Conversation history\n",
    "- Context preservation\n",
    "- Session state management\n",
    "\n",
    "#### Next Steps\n",
    "Focus on context management system and turn detection improvements for Phase 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP2 - Turn Detection, better VAD and Silence Detection \n",
    "\n",
    "#### Voice Activity Detection (VAD)\n",
    "- Configurable VAD threshold\n",
    "- Proper silence duration tracking\n",
    "- Minimum speech frames validation\n",
    "\n",
    "#### Audio Processing\n",
    "- Block-based processing (200ms blocks)\n",
    "- Efficient buffer management\n",
    "- Real-time audio level monitoring\n",
    "\n",
    "#### State Management\n",
    "- Clear speech state tracking\n",
    "- Better interruption handling\n",
    "- Dynamic silence detection\n",
    "\n",
    "Expected outcome: More natural conversation flow with reliable speech detection and turn-taking.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import base64\n",
    "import json\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import websockets\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "class AudioProcessor:\n",
    "    def __init__(self, sample_rate=24000):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.vad_threshold = 0.015\n",
    "        self.speech_frames = 0\n",
    "        self.silence_frames = 0\n",
    "        self.min_speech_duration = int(0.3 * sample_rate)\n",
    "        self.max_silence_duration = int(0.8 * sample_rate)\n",
    "        self.buffer = []\n",
    "        self.is_speaking = False\n",
    "        self.speech_detected = False\n",
    "\n",
    "    def process_audio(self, indata):\n",
    "        if self.is_speaking:\n",
    "            return\n",
    "\n",
    "        audio_level = np.abs(indata).mean() / 32768.0\n",
    "        \n",
    "        if audio_level > self.vad_threshold:\n",
    "            self.speech_detected = True\n",
    "            self.speech_frames += len(indata)\n",
    "            self.silence_frames = 0\n",
    "            self.buffer.extend(indata.tobytes())\n",
    "        elif self.speech_detected:\n",
    "            self.silence_frames += len(indata)\n",
    "            if self.silence_frames < self.max_silence_duration:\n",
    "                self.buffer.extend(indata.tobytes())\n",
    "\n",
    "    def should_process(self):\n",
    "        return (self.speech_detected and \n",
    "                self.speech_frames >= self.min_speech_duration and \n",
    "                self.silence_frames >= self.max_silence_duration)\n",
    "\n",
    "    def reset(self):\n",
    "        self.speech_frames = 0\n",
    "        self.silence_frames = 0\n",
    "        self.speech_detected = False\n",
    "        audio_data = bytes(self.buffer)\n",
    "        self.buffer.clear()\n",
    "        return audio_data\n",
    "\n",
    "class ConversationSystem:\n",
    "    def __init__(self):\n",
    "        load_dotenv()\n",
    "        self.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"AZURE_OPENAI_API_KEY not found\")\n",
    "            \n",
    "        self.url = (\n",
    "            \"wss://aoai-ep-swedencentral02.openai.azure.com/openai/realtime?\"\n",
    "            f\"api-version=2024-10-01-preview&deployment=gpt-4o-realtime-preview&\"\n",
    "            f\"api-key={self.api_key}\"\n",
    "        )\n",
    "        \n",
    "        self.audio_processor = AudioProcessor()\n",
    "        self.streams = {'input': None, 'output': None}\n",
    "\n",
    "    def audio_callback(self, indata, frames, time, status):\n",
    "        if status:\n",
    "            print(f\"Audio error: {status}\")\n",
    "            return\n",
    "        self.audio_processor.process_audio(indata)\n",
    "\n",
    "    async def setup_audio(self):\n",
    "        self.streams['output'] = sd.OutputStream(\n",
    "            samplerate=24000, channels=1, dtype=np.int16)\n",
    "        self.streams['input'] = sd.InputStream(\n",
    "            samplerate=24000, channels=1, dtype=np.int16,\n",
    "            callback=self.audio_callback, blocksize=4800)\n",
    "            \n",
    "        for stream in self.streams.values():\n",
    "            stream.start()\n",
    "\n",
    "    async def setup_websocket_session(self, websocket):\n",
    "        session_config = {\n",
    "            \"type\": \"session.update\",\n",
    "            \"session\": {\n",
    "                \"voice\": \"alloy\",\n",
    "                \"instructions\": \"You are a helpful AI assistant. Keep responses brief.\",\n",
    "                \"modalities\": [\"audio\", \"text\"],\n",
    "                \"input_audio_format\": \"pcm16\",\n",
    "                \"output_audio_format\": \"pcm16\",\n",
    "                \"turn_detection\": {\n",
    "                    \"type\": \"server_vad\",\n",
    "                    \"threshold\": 0.3,\n",
    "                    \"prefix_padding_ms\": 150,\n",
    "                    \"silence_duration_ms\": 600\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        await websocket.send(json.dumps(session_config))\n",
    "        \n",
    "        while True:\n",
    "            response = json.loads(await websocket.recv())\n",
    "            if response[\"type\"] == \"session.created\":\n",
    "                break\n",
    "            if response[\"type\"] == \"error\":\n",
    "                raise Exception(f\"Session setup failed: {response}\")\n",
    "\n",
    "    async def send_audio(self, websocket, audio_data):\n",
    "        audio_base64 = base64.b64encode(audio_data).decode('utf-8')\n",
    "        \n",
    "        await websocket.send(json.dumps({\n",
    "            \"type\": \"input_audio_buffer.append\",\n",
    "            \"audio\": audio_base64\n",
    "        }))\n",
    "        await websocket.send(json.dumps({\"type\": \"input_audio_buffer.commit\"}))\n",
    "        await websocket.send(json.dumps({\n",
    "            \"type\": \"response.create\",\n",
    "            \"response\": {\"modalities\": [\"audio\", \"text\"]}\n",
    "        }))\n",
    "\n",
    "    async def handle_response(self, websocket):\n",
    "        self.audio_processor.is_speaking = True\n",
    "        \n",
    "        try:\n",
    "            while True:\n",
    "                response = json.loads(await websocket.recv())\n",
    "                \n",
    "                if response[\"type\"] == \"response.audio.delta\":\n",
    "                    if \"delta\" in response:\n",
    "                        try:\n",
    "                            audio_data = response[\"delta\"].strip()\n",
    "                            padding = -len(audio_data) % 4\n",
    "                            if padding:\n",
    "                                audio_data += \"=\" * padding\n",
    "                            \n",
    "                            audio = np.frombuffer(\n",
    "                                base64.b64decode(audio_data), \n",
    "                                dtype=np.int16\n",
    "                            )\n",
    "                            self.streams['output'].write(audio)\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"Audio processing error: {e}\")\n",
    "                            \n",
    "                elif response[\"type\"] == \"response.done\":\n",
    "                    break\n",
    "                    \n",
    "        finally:\n",
    "            self.audio_processor.is_speaking = False\n",
    "\n",
    "    async def run(self):\n",
    "        await self.setup_audio()\n",
    "        \n",
    "        async with websockets.connect(self.url) as ws:\n",
    "            await self.setup_websocket_session(ws)\n",
    "            print(\"Ready for conversation\")\n",
    "            \n",
    "            while True:\n",
    "                if self.audio_processor.should_process():\n",
    "                    audio_data = self.audio_processor.reset()\n",
    "                    await self.send_audio(ws, audio_data)\n",
    "                    await self.handle_response(ws)\n",
    "                await asyncio.sleep(0.05)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    system = ConversationSystem()\n",
    "    asyncio.run(system.run())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes & Expected Effects\n",
    "\n",
    "#### VAD Parameters\n",
    "- Changed threshold: 500 -> 0.015 (normalized)\n",
    "- Effect: More sensitive speech detection\n",
    "\n",
    "#### Timing\n",
    "- Sleep interval: 0.1s -> 0.05s\n",
    "- Effect: Faster response time\n",
    "\n",
    "#### Buffer Management\n",
    "- Added proper reset after processing\n",
    "- Effect: Prevents audio data buildup\n",
    "\n",
    "#### State Tracking\n",
    "- Added speech_frames counter\n",
    "- Min speech duration: 0.3s\n",
    "- Max silence: 0.8s\n",
    "- Effect: Better conversation turn detection\n",
    "\n",
    "#### Code Structure\n",
    "- Split into AudioProcessor class\n",
    "- Effect: Cleaner maintenance, better testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Here is a working script](https://github.com/ozgurgulerx/ai-builders/blob/main/part1_build/p1uc1_realtime_api_converse_step2.py) \n",
    "\n",
    "Now the bot is more responsive and can detect the user's speech more accurately. \\\n",
    "The conversation has become more fluid and natural. \\\n",
    "Well done! \n",
    "\n",
    "One missing piece is improving \"interrupt handling\". Voice bot keeps going even when we interrupt - we will address this in the next step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Implementing Effective Interruption Handling in Voice Conversation Systems\n",
    "\n",
    "#### Understanding the Challenge\n",
    "\n",
    "Interruption handling in voice conversation systems requires careful consideration of both technical and user experience aspects. When a human interrupts an AI system mid-response, they expect the system to behave similarly to a human conversation partner - stopping the current topic and paying attention to the new input.\n",
    "\n",
    "#### Core Requirements\n",
    "\n",
    "##### 1. Speech Detection During System Output\n",
    "\n",
    "The system must actively monitor user audio even while it's speaking. This requires:\n",
    "- Maintaining an audio buffer of recent input\n",
    "- Continuously analyzing audio levels\n",
    "- Setting appropriate threshold values that can distinguish between background noise and intentional speech\n",
    "- Implementing this without impacting system performance\n",
    "\n",
    "##### 2. Immediate Response Cancellation\n",
    "\n",
    "When interruption is detected, the system should:\n",
    "- Stop the current audio output immediately\n",
    "- Cancel any pending response generation\n",
    "- Provide feedback to the user that their interruption was recognized\n",
    "- Complete this process quickly enough to feel natural\n",
    "\n",
    "##### 3. Context Management\n",
    "\n",
    "The most crucial aspect of interruption handling is proper context management:\n",
    "- The system must clear its current conversation context\n",
    "- Previous topic and response should be completely abandoned\n",
    "- New user input should be treated as a fresh conversation start\n",
    "- The system should be ready to process the new topic immediately\n",
    "\n",
    "##### 4. Natural Conversation Flow\n",
    "\n",
    "To maintain natural interaction:\n",
    "- The interruption threshold should be carefully calibrated\n",
    "- False positives (detecting interruptions when there aren't any) should be minimized\n",
    "- The system should respond quickly to genuine interruptions\n",
    "- The transition to the new topic should feel smooth and natural\n",
    "\n",
    "#### Implementation Considerations\n",
    "\n",
    "When implementing these features, consider:\n",
    "- Audio processing overhead and its impact on system performance\n",
    "- Balance between sensitivity and accuracy in interruption detection\n",
    "- Proper cleanup of system resources during interruption\n",
    "- Error handling for various edge cases\n",
    "- Testing with different voices and background conditions\n",
    "\n",
    "#### Success Metrics\n",
    "\n",
    "A well-implemented interruption system should:\n",
    "- Detect interruptions within 200-300ms\n",
    "- Successfully abandon previous context\n",
    "- Process new input correctly\n",
    "- Maintain conversation fluidity\n",
    "- Handle rapid topic switches naturally\n",
    "\n",
    "Understanding and implementing these aspects will create a more natural and engaging voice interaction system that better matches human conversation patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import base64\n",
    "import json\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import websockets\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "class AudioProcessor:\n",
    "    def __init__(self, sample_rate=24000):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.vad_threshold = 0.015\n",
    "        self.interrupt_threshold = 0.02\n",
    "        self.speech_frames = 0\n",
    "        self.silence_frames = 0\n",
    "        self.min_speech_duration = int(0.3 * sample_rate)\n",
    "        self.max_silence_duration = int(0.8 * sample_rate)\n",
    "        self.buffer = []\n",
    "        self.is_speaking = False\n",
    "        self.speech_detected = False\n",
    "        self.latest_audio = None\n",
    "        self.was_interrupted = False  # Track interruption state\n",
    "\n",
    "    def process_audio(self, indata):\n",
    "        self.latest_audio = indata\n",
    "        if self.is_speaking:\n",
    "            return\n",
    "\n",
    "        audio_level = np.abs(indata).mean() / 32768.0\n",
    "        \n",
    "        if audio_level > self.vad_threshold:\n",
    "            self.speech_detected = True\n",
    "            self.speech_frames += len(indata)\n",
    "            self.silence_frames = 0\n",
    "            self.buffer.extend(indata.tobytes())\n",
    "        elif self.speech_detected:\n",
    "            self.silence_frames += len(indata)\n",
    "            if self.silence_frames < self.max_silence_duration:\n",
    "                self.buffer.extend(indata.tobytes())\n",
    "\n",
    "    def check_interruption(self):\n",
    "        if not self.is_speaking or self.latest_audio is None:\n",
    "            return False\n",
    "        audio_level = np.abs(self.latest_audio).mean() / 32768.0\n",
    "        return audio_level > self.interrupt_threshold\n",
    "\n",
    "    def should_process(self):\n",
    "        return (self.speech_detected and \n",
    "                self.speech_frames >= self.min_speech_duration and \n",
    "                self.silence_frames >= self.max_silence_duration)\n",
    "\n",
    "    def reset(self):\n",
    "        self.speech_frames = 0\n",
    "        self.silence_frames = 0\n",
    "        self.speech_detected = False\n",
    "        audio_data = bytes(self.buffer)\n",
    "        self.buffer.clear()\n",
    "        return audio_data\n",
    "\n",
    "class ConversationSystem:\n",
    "    def __init__(self):\n",
    "        load_dotenv()\n",
    "        self.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"AZURE_OPENAI_API_KEY not found\")\n",
    "            \n",
    "        self.url = (\n",
    "            \"wss://aoai-ep-swedencentral02.openai.azure.com/openai/realtime?\"\n",
    "            f\"api-version=2024-10-01-preview&deployment=gpt-4o-realtime-preview&\"\n",
    "            f\"api-key={self.api_key}\"\n",
    "        )\n",
    "        \n",
    "        self.audio_processor = AudioProcessor()\n",
    "        self.streams = {'input': None, 'output': None}\n",
    "\n",
    "    def audio_callback(self, indata, frames, time, status):\n",
    "        if status:\n",
    "            print(f\"Audio error: {status}\")\n",
    "            return\n",
    "        self.audio_processor.process_audio(indata)\n",
    "\n",
    "    async def setup_audio(self):\n",
    "        self.streams['output'] = sd.OutputStream(\n",
    "            samplerate=24000, channels=1, dtype=np.int16)\n",
    "        self.streams['input'] = sd.InputStream(\n",
    "            samplerate=24000, channels=1, dtype=np.int16,\n",
    "            callback=self.audio_callback, blocksize=4800)\n",
    "            \n",
    "        for stream in self.streams.values():\n",
    "            stream.start()\n",
    "\n",
    "    async def setup_websocket_session(self, websocket):\n",
    "        session_config = {\n",
    "            \"type\": \"session.update\",\n",
    "            \"session\": {\n",
    "                \"voice\": \"alloy\",\n",
    "                \"instructions\": \"You are a helpful AI assistant. Keep responses brief.\",\n",
    "                \"modalities\": [\"audio\", \"text\"],\n",
    "                \"input_audio_format\": \"pcm16\",\n",
    "                \"output_audio_format\": \"pcm16\",\n",
    "                \"turn_detection\": {\n",
    "                    \"type\": \"server_vad\",\n",
    "                    \"threshold\": 0.3,\n",
    "                    \"prefix_padding_ms\": 150,\n",
    "                    \"silence_duration_ms\": 600\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        await websocket.send(json.dumps(session_config))\n",
    "        \n",
    "        while True:\n",
    "            response = json.loads(await websocket.recv())\n",
    "            if response[\"type\"] == \"session.created\":\n",
    "                break\n",
    "            if response[\"type\"] == \"error\":\n",
    "                raise Exception(f\"Session setup failed: {response}\")\n",
    "\n",
    "    async def handle_interruption(self, websocket):\n",
    "        print(\"Interrupted!\")\n",
    "        # First cancel the current response\n",
    "        await websocket.send(json.dumps({\n",
    "            \"type\": \"response.cancel\"\n",
    "        }))\n",
    "        # Then clear the conversation context\n",
    "        await websocket.send(json.dumps({\n",
    "            \"type\": \"conversation.item.truncate\",\n",
    "            \"position\": 0  # Clear all items\n",
    "        }))\n",
    "        self.audio_processor.was_interrupted = True\n",
    "\n",
    "    async def send_audio(self, websocket, audio_data):\n",
    "        audio_base64 = base64.b64encode(audio_data).decode('utf-8')\n",
    "        \n",
    "        await websocket.send(json.dumps({\n",
    "            \"type\": \"input_audio_buffer.append\",\n",
    "            \"audio\": audio_base64\n",
    "        }))\n",
    "        await websocket.send(json.dumps({\"type\": \"input_audio_buffer.commit\"}))\n",
    "        await websocket.send(json.dumps({\n",
    "            \"type\": \"response.create\",\n",
    "            \"response\": {\"modalities\": [\"audio\", \"text\"]}\n",
    "        }))\n",
    "\n",
    "    async def handle_response(self, websocket):\n",
    "        self.audio_processor.is_speaking = True\n",
    "        try:\n",
    "            while True:\n",
    "                if self.audio_processor.check_interruption():\n",
    "                    await self.handle_interruption(websocket)\n",
    "                    break\n",
    "                \n",
    "                response = json.loads(await websocket.recv())\n",
    "                \n",
    "                if response[\"type\"] == \"response.audio.delta\":\n",
    "                    if \"delta\" in response:\n",
    "                        try:\n",
    "                            audio_data = response[\"delta\"].strip()\n",
    "                            padding = -len(audio_data) % 4\n",
    "                            if padding:\n",
    "                                audio_data += \"=\" * padding\n",
    "                            \n",
    "                            audio = np.frombuffer(\n",
    "                                base64.b64decode(audio_data), \n",
    "                                dtype=np.int16\n",
    "                            )\n",
    "                            self.streams['output'].write(audio)\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"Audio processing error: {e}\")\n",
    "                            \n",
    "                elif response[\"type\"] == \"response.done\":\n",
    "                    break\n",
    "                    \n",
    "        finally:\n",
    "            self.audio_processor.is_speaking = False\n",
    "\n",
    "    async def run(self):\n",
    "        await self.setup_audio()\n",
    "        \n",
    "        async with websockets.connect(self.url) as ws:\n",
    "            await self.setup_websocket_session(ws)\n",
    "            print(\"Ready for conversation\")\n",
    "            \n",
    "            while True:\n",
    "                if self.audio_processor.should_process():\n",
    "                    audio_data = self.audio_processor.reset()\n",
    "                    await self.send_audio(ws, audio_data)\n",
    "                    await self.handle_response(ws)\n",
    "                await asyncio.sleep(0.05)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    system = ConversationSystem()\n",
    "    asyncio.run(system.run())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
