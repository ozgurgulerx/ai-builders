{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONVERSE with REALTIME API \n",
    "In this step, we'll transform our single-turn audio generation into a full conversational system. This involves tackling several key challenges: managing bi-directional audio streams for both user input and AI output, implementing proper turn-taking mechanics (knowing when the user has finished speaking and when the AI should respond), handling the complex flow of events from speech detection to response generation, and maintaining conversation context across multiple exchanges. The trickiest part isn't just streaming the audio - it's orchestrating all these components to work together smoothly while maintaining natural conversational flow. We'll build this step by step, focusing on creating a robust system that can handle real-world conversation scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Challenges in Building Real-time Conversational AI\n",
    "\n",
    "Below are the key challanges in building Real-time Conversational AI...\n",
    "\n",
    "1. **Session and Context Management**\n",
    "  - Maintaining conversation history\n",
    "  - Tracking context across exchanges\n",
    "  - Managing session state\n",
    "\n",
    "2. **Turn-Taking Mechanics**\n",
    "  - Detecting end of user speech\n",
    "  - Managing interruptions\n",
    "  - Coordinating input/output transitions\n",
    "  - Timing audio recording and playback\n",
    "\n",
    "3. **Audio Stream Handling**\n",
    "  - Coordinating bidirectional audio\n",
    "  - Managing stream lifecycles\n",
    "  - Resolving device conflicts\n",
    "  - Handling format compatibility\n",
    "\n",
    "4. **Event Flow Control**\n",
    "  - Input audio streaming\n",
    "  - Speech detection\n",
    "  - Text transcription\n",
    "  - Response generation\n",
    "  - Output audio streaming\n",
    "\n",
    "5. **Error Recovery**\n",
    "  - Connection drops\n",
    "  - Audio device issues\n",
    "  - API rate limits\n",
    "  - Graceful error handling\n",
    "\n",
    "6. **Resource Management**\n",
    "  - WebSocket connection handling\n",
    "  - Audio buffer memory\n",
    "  - Resource cleanup\n",
    "  - Connection lifecycle management\n",
    "\n",
    "\n",
    "We will build an MVP, therefore choose the minimum set that will give us a working setup. The below is what we will target. \n",
    "\n",
    "### Minimum Viable Conversational System - Development Plan\n",
    "\n",
    "#### STEP 1 **Session and Context Management**\n",
    "#### STEP 2 **Turn Detection, better VAD and Silence Detection**\n",
    "#### STEP 3 **Interruption Handling**\n",
    "#### STEP 4 **Session and Context Management**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1 **Session and Context Management**\n",
    "We will implement the below key features implemented:\n",
    "\n",
    "- Bidirectional audio streams\n",
    "- Basic turn management\n",
    "- Audio buffering\n",
    "- Session handling\n",
    "- Basic error recovery\n",
    "\n",
    "Finally test by running the script - it will listen for 2 seconds, send to API, then play response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import base64\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import websockets\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "\n",
    "class ConversationSystem:\n",
    "    def __init__(self):\n",
    "        load_dotenv()\n",
    "        self.input_stream = None\n",
    "        self.output_stream = None\n",
    "        self.is_speaking = False\n",
    "        self.input_buffer = []\n",
    "        self.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"AZURE_OPENAI_API_KEY not found in environment\")\n",
    "        self.url = (\n",
    "            f\"wss://aoai-ep-swedencentral02.openai.azure.com/openai/realtime?\"\n",
    "            f\"api-version=2024-10-01-preview&deployment=gpt-4o-realtime-preview&api-key={self.api_key}\"\n",
    "        )\n",
    "\n",
    "    async def setup_audio(self):\n",
    "        print(\"Setting up audio streams...\")\n",
    "        self.output_stream = sd.OutputStream(samplerate=24000, channels=1, dtype=np.int16)\n",
    "        self.input_stream = sd.InputStream(samplerate=24000, channels=1, dtype=np.int16,\n",
    "                                         callback=self.audio_callback)\n",
    "        self.output_stream.start()\n",
    "        self.input_stream.start()\n",
    "        print(\"Audio streams started\")\n",
    "        \n",
    "    def audio_callback(self, indata, frames, time, status):\n",
    "        if status:\n",
    "            print(f\"Input stream error: {status}\")\n",
    "        if not self.is_speaking:\n",
    "            self.input_buffer.extend(indata.tobytes())\n",
    "\n",
    "    async def start_conversation(self):\n",
    "        try:\n",
    "            async with websockets.connect(self.url) as ws:\n",
    "                print(\"Connected to WebSocket\")\n",
    "                await self.setup_session(ws)\n",
    "                \n",
    "                # Initial greeting\n",
    "                await self.send_message(ws, \"Hello\")\n",
    "                await self.handle_response(ws)\n",
    "\n",
    "                while True:\n",
    "                    print(\"\\nListening... (speak for at least 2 seconds)\")\n",
    "                    self.input_buffer.clear()\n",
    "                    await asyncio.sleep(2)  # Wait for 2 seconds of audio\n",
    "\n",
    "                    if len(self.input_buffer) > 0:\n",
    "                        print(\"Processing your input...\")\n",
    "                        audio_data = bytes(self.input_buffer)\n",
    "                        self.input_buffer.clear()\n",
    "                        \n",
    "                        # Send audio\n",
    "                        base64_audio = base64.b64encode(audio_data).decode('utf-8')\n",
    "                        await ws.send(json.dumps({\n",
    "                            \"type\": \"input_audio_buffer.append\",\n",
    "                            \"audio\": base64_audio\n",
    "                        }))\n",
    "                        await ws.send(json.dumps({\n",
    "                            \"type\": \"input_audio_buffer.commit\"\n",
    "                        }))\n",
    "                        \n",
    "                        # Request response\n",
    "                        await ws.send(json.dumps({\n",
    "                            \"type\": \"response.create\",\n",
    "                            \"response\": {\"modalities\": [\"audio\", \"text\"]}\n",
    "                        }))\n",
    "                        \n",
    "                        await self.handle_response(ws)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in conversation: {e}\")\n",
    "\n",
    "    async def setup_session(self, ws):\n",
    "        session_payload = {\n",
    "            \"type\": \"session.update\",\n",
    "            \"session\": {\n",
    "                \"voice\": \"alloy\",\n",
    "                \"instructions\": \"You are a helpful AI assistant. Keep responses brief and engaging.\",\n",
    "                \"modalities\": [\"audio\", \"text\"],\n",
    "                \"input_audio_format\": \"pcm16\",\n",
    "                \"output_audio_format\": \"pcm16\",\n",
    "                \"turn_detection\": {\n",
    "                    \"type\": \"server_vad\",\n",
    "                    \"threshold\": 0.5,\n",
    "                    \"prefix_padding_ms\": 300,\n",
    "                    \"silence_duration_ms\": 200\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        await ws.send(json.dumps(session_payload))\n",
    "        \n",
    "        while True:\n",
    "            response = await ws.recv()\n",
    "            data = json.loads(response)\n",
    "            if data.get(\"type\") == \"session.created\":\n",
    "                print(\"Session setup complete\")\n",
    "                break\n",
    "            elif data.get(\"type\") == \"error\":\n",
    "                raise Exception(\"Error creating session\")\n",
    "\n",
    "    async def send_message(self, ws, text):\n",
    "        message_payload = {\n",
    "            \"type\": \"conversation.item.create\",\n",
    "            \"item\": {\n",
    "                \"type\": \"message\",\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"type\": \"input_text\", \"text\": text}]\n",
    "            }\n",
    "        }\n",
    "        await ws.send(json.dumps(message_payload))\n",
    "        \n",
    "        await ws.send(json.dumps({\n",
    "            \"type\": \"response.create\",\n",
    "            \"response\": {\"modalities\": [\"audio\", \"text\"]}\n",
    "        }))\n",
    "\n",
    "    async def handle_response(self, ws):\n",
    "        self.is_speaking = True\n",
    "        try:\n",
    "            while True:\n",
    "                response = await ws.recv()\n",
    "                data = json.loads(response)\n",
    "                \n",
    "                if data[\"type\"] == \"response.audio.delta\":\n",
    "                    if \"delta\" in data:\n",
    "                        try:\n",
    "                            audio_data = data[\"delta\"].replace(\" \", \"\").replace(\"\\n\", \"\")\n",
    "                            padding = len(audio_data) % 4\n",
    "                            if padding:\n",
    "                                audio_data += \"=\" * padding\n",
    "                            \n",
    "                            audio_bytes = base64.b64decode(audio_data)\n",
    "                            audio = np.frombuffer(audio_bytes, dtype=np.int16)\n",
    "                            self.output_stream.write(audio)\n",
    "                            print(\".\", end=\"\", flush=True)\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing audio: {e}\")\n",
    "                            \n",
    "                elif data[\"type\"] == \"response.done\":\n",
    "                    break\n",
    "                    \n",
    "        finally:\n",
    "            self.is_speaking = False\n",
    "\n",
    "async def main():\n",
    "    system = ConversationSystem()\n",
    "    await system.setup_audio()\n",
    "    await system.start_conversation()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting real-time conversation system...\")\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the above as a python script outside of the notebook as event loops clash with Jupyters scheduling. \\\n",
    "[Here is a working script](https://github.com/ozgurgulerx/ai-builders/blob/main/part1_build/p1uc1_realtime_api_converse_step1.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila - our voice chatbot works!!! \\\n",
    "Still a bit clumsy though in terms of detecting session dynamics as well as properly listenning and understanding the user. \\\n",
    "We will improve it in the next steps.\n",
    "\n",
    "Implementation Status: Phase 1 \\\n",
    "\n",
    "#### ✓ Completed\n",
    "- Audio I/O setup (device configuration, streaming)\n",
    "- Server-side VAD implementation\n",
    "- Basic audio state management\n",
    "\n",
    "#### 🔄 Partially Implemented\n",
    "- Turn detection (needs refinement)\n",
    "- Basic interrupt handling\n",
    "- State tracking\n",
    "\n",
    "##### ❌ Not Implemented\n",
    "- Conversation history\n",
    "- Context preservation\n",
    "- Session state management\n",
    "\n",
    "#### Next Steps\n",
    "Focus on context management system and turn detection improvements for Phase 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP2 - Turn Detection, better VAD and Silence Detection \n",
    "\n",
    "#### Voice Activity Detection (VAD)\n",
    "- Configurable VAD threshold\n",
    "- Proper silence duration tracking\n",
    "- Minimum speech frames validation\n",
    "\n",
    "#### Audio Processing\n",
    "- Block-based processing (200ms blocks)\n",
    "- Efficient buffer management\n",
    "- Real-time audio level monitoring\n",
    "\n",
    "#### State Management\n",
    "- Clear speech state tracking\n",
    "- Better interruption handling\n",
    "- Dynamic silence detection\n",
    "\n",
    "Expected outcome: More natural conversation flow with reliable speech detection and turn-taking.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import base64\n",
    "import json\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import websockets\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "class AudioProcessor:\n",
    "    def __init__(self, sample_rate=24000):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.vad_threshold = 0.015\n",
    "        self.speech_frames = 0\n",
    "        self.silence_frames = 0\n",
    "        self.min_speech_duration = int(0.3 * sample_rate)\n",
    "        self.max_silence_duration = int(0.8 * sample_rate)\n",
    "        self.buffer = []\n",
    "        self.is_speaking = False\n",
    "        self.speech_detected = False\n",
    "\n",
    "    def process_audio(self, indata):\n",
    "        if self.is_speaking:\n",
    "            return\n",
    "\n",
    "        audio_level = np.abs(indata).mean() / 32768.0\n",
    "        \n",
    "        if audio_level > self.vad_threshold:\n",
    "            self.speech_detected = True\n",
    "            self.speech_frames += len(indata)\n",
    "            self.silence_frames = 0\n",
    "            self.buffer.extend(indata.tobytes())\n",
    "        elif self.speech_detected:\n",
    "            self.silence_frames += len(indata)\n",
    "            if self.silence_frames < self.max_silence_duration:\n",
    "                self.buffer.extend(indata.tobytes())\n",
    "\n",
    "    def should_process(self):\n",
    "        return (self.speech_detected and \n",
    "                self.speech_frames >= self.min_speech_duration and \n",
    "                self.silence_frames >= self.max_silence_duration)\n",
    "\n",
    "    def reset(self):\n",
    "        self.speech_frames = 0\n",
    "        self.silence_frames = 0\n",
    "        self.speech_detected = False\n",
    "        audio_data = bytes(self.buffer)\n",
    "        self.buffer.clear()\n",
    "        return audio_data\n",
    "\n",
    "class ConversationSystem:\n",
    "    def __init__(self):\n",
    "        load_dotenv()\n",
    "        self.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"AZURE_OPENAI_API_KEY not found\")\n",
    "            \n",
    "        self.url = (\n",
    "            \"wss://aoai-ep-swedencentral02.openai.azure.com/openai/realtime?\"\n",
    "            f\"api-version=2024-10-01-preview&deployment=gpt-4o-realtime-preview&\"\n",
    "            f\"api-key={self.api_key}\"\n",
    "        )\n",
    "        \n",
    "        self.audio_processor = AudioProcessor()\n",
    "        self.streams = {'input': None, 'output': None}\n",
    "\n",
    "    def audio_callback(self, indata, frames, time, status):\n",
    "        if status:\n",
    "            print(f\"Audio error: {status}\")\n",
    "            return\n",
    "        self.audio_processor.process_audio(indata)\n",
    "\n",
    "    async def setup_audio(self):\n",
    "        self.streams['output'] = sd.OutputStream(\n",
    "            samplerate=24000, channels=1, dtype=np.int16)\n",
    "        self.streams['input'] = sd.InputStream(\n",
    "            samplerate=24000, channels=1, dtype=np.int16,\n",
    "            callback=self.audio_callback, blocksize=4800)\n",
    "            \n",
    "        for stream in self.streams.values():\n",
    "            stream.start()\n",
    "\n",
    "    async def setup_websocket_session(self, websocket):\n",
    "        session_config = {\n",
    "            \"type\": \"session.update\",\n",
    "            \"session\": {\n",
    "                \"voice\": \"alloy\",\n",
    "                \"instructions\": \"You are a helpful AI assistant. Keep responses brief.\",\n",
    "                \"modalities\": [\"audio\", \"text\"],\n",
    "                \"input_audio_format\": \"pcm16\",\n",
    "                \"output_audio_format\": \"pcm16\",\n",
    "                \"turn_detection\": {\n",
    "                    \"type\": \"server_vad\",\n",
    "                    \"threshold\": 0.3,\n",
    "                    \"prefix_padding_ms\": 150,\n",
    "                    \"silence_duration_ms\": 600\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        await websocket.send(json.dumps(session_config))\n",
    "        \n",
    "        while True:\n",
    "            response = json.loads(await websocket.recv())\n",
    "            if response[\"type\"] == \"session.created\":\n",
    "                break\n",
    "            if response[\"type\"] == \"error\":\n",
    "                raise Exception(f\"Session setup failed: {response}\")\n",
    "\n",
    "    async def send_audio(self, websocket, audio_data):\n",
    "        audio_base64 = base64.b64encode(audio_data).decode('utf-8')\n",
    "        \n",
    "        await websocket.send(json.dumps({\n",
    "            \"type\": \"input_audio_buffer.append\",\n",
    "            \"audio\": audio_base64\n",
    "        }))\n",
    "        await websocket.send(json.dumps({\"type\": \"input_audio_buffer.commit\"}))\n",
    "        await websocket.send(json.dumps({\n",
    "            \"type\": \"response.create\",\n",
    "            \"response\": {\"modalities\": [\"audio\", \"text\"]}\n",
    "        }))\n",
    "\n",
    "    async def handle_response(self, websocket):\n",
    "        self.audio_processor.is_speaking = True\n",
    "        \n",
    "        try:\n",
    "            while True:\n",
    "                response = json.loads(await websocket.recv())\n",
    "                \n",
    "                if response[\"type\"] == \"response.audio.delta\":\n",
    "                    if \"delta\" in response:\n",
    "                        try:\n",
    "                            audio_data = response[\"delta\"].strip()\n",
    "                            padding = -len(audio_data) % 4\n",
    "                            if padding:\n",
    "                                audio_data += \"=\" * padding\n",
    "                            \n",
    "                            audio = np.frombuffer(\n",
    "                                base64.b64decode(audio_data), \n",
    "                                dtype=np.int16\n",
    "                            )\n",
    "                            self.streams['output'].write(audio)\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"Audio processing error: {e}\")\n",
    "                            \n",
    "                elif response[\"type\"] == \"response.done\":\n",
    "                    break\n",
    "                    \n",
    "        finally:\n",
    "            self.audio_processor.is_speaking = False\n",
    "\n",
    "    async def run(self):\n",
    "        await self.setup_audio()\n",
    "        \n",
    "        async with websockets.connect(self.url) as ws:\n",
    "            await self.setup_websocket_session(ws)\n",
    "            print(\"Ready for conversation\")\n",
    "            \n",
    "            while True:\n",
    "                if self.audio_processor.should_process():\n",
    "                    audio_data = self.audio_processor.reset()\n",
    "                    await self.send_audio(ws, audio_data)\n",
    "                    await self.handle_response(ws)\n",
    "                await asyncio.sleep(0.05)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    system = ConversationSystem()\n",
    "    asyncio.run(system.run())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes & Expected Effects\n",
    "\n",
    "#### VAD Parameters\n",
    "- Changed threshold: 500 -> 0.015 (normalized)\n",
    "- Effect: More sensitive speech detection\n",
    "\n",
    "#### Timing\n",
    "- Sleep interval: 0.1s -> 0.05s\n",
    "- Effect: Faster response time\n",
    "\n",
    "#### Buffer Management\n",
    "- Added proper reset after processing\n",
    "- Effect: Prevents audio data buildup\n",
    "\n",
    "#### State Tracking\n",
    "- Added speech_frames counter\n",
    "- Min speech duration: 0.3s\n",
    "- Max silence: 0.8s\n",
    "- Effect: Better conversation turn detection\n",
    "\n",
    "#### Code Structure\n",
    "- Split into AudioProcessor class\n",
    "- Effect: Cleaner maintenance, better testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Here is a working script](https://github.com/ozgurgulerx/ai-builders/blob/main/part1_build/p1uc1_realtime_api_converse_step2.py) \n",
    "\n",
    "Now the bot is more responsive and can detect the user's speech more accurately. \\\n",
    "The conversation has become more fluid and natural. \\\n",
    "Well done! \n",
    "\n",
    "One missing piece is improving \"interrupt handling\". Voice bot keeps going even when we interrupt - we will address this in the next step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3: Interruption Handling \n",
    "\n",
    "**Initial Challenge: Beyond Basic Interruption Detection**\n",
    "\n",
    "The Azure OpenAI SDK documentation reveals that effective interruption handling requires more than just detecting and stopping speech. We must properly manage the full conversation context and message flow to create natural interactions.\n",
    "\n",
    "**Key Components for Successful Implementation**\n",
    "\n",
    "**1. Conversation Context Management**\n",
    "The system needs to actively track and manage conversation state through:\n",
    "- Unique conversation IDs for each interaction session\n",
    "- Message IDs for individual exchanges\n",
    "- Proper association between audio inputs and their conversation context\n",
    "\n",
    "**2. Interruption Audio Processing**\n",
    "When interruption occurs, the system must:\n",
    "- Maintain a buffer of the interrupting speech\n",
    "- Process this speech as a new conversation input\n",
    "- Ensure no audio information is lost during the transition\n",
    "\n",
    "**3. Structured Message Flow**\n",
    "The conversation must follow a specific structure:\n",
    "- Create a new conversation message before sending audio\n",
    "- Associate audio data with the correct message ID\n",
    "- Maintain proper sequencing of conversation turns\n",
    "\n",
    "**Implementation Requirements**\n",
    "\n",
    "**Technical Infrastructure**\n",
    "- Audio buffer management system\n",
    "- Continuous audio level monitoring\n",
    "- Calibrated threshold values\n",
    "- Performance-optimized processing\n",
    "\n",
    "**Conversation Flow Control**\n",
    "- Immediate response cancellation capability\n",
    "- Context clearing mechanisms\n",
    "- New conversation message creation\n",
    "- Proper message sequencing\n",
    "\n",
    "**User Experience Considerations**\n",
    "- Quick interruption detection (200-300ms)\n",
    "- Minimal false positives\n",
    "- Smooth topic transitions\n",
    "- Natural conversation rhythm\n",
    "\n",
    "**Process Flow During Interruption**\n",
    "\n",
    "1. Initial Detection\n",
    "  - Monitor audio levels during system speech\n",
    "  - Compare against interruption threshold\n",
    "  - Trigger interruption handling when threshold is exceeded\n",
    "\n",
    "2. Response Management\n",
    "  - Cancel current audio output\n",
    "  - Clear ongoing response generation\n",
    "  - Preserve interrupting audio\n",
    "\n",
    "3. Context Transition\n",
    "  - Clear existing conversation context\n",
    "  - Create new conversation message\n",
    "  - Associate preserved audio with new message\n",
    "\n",
    "4. New Input Processing\n",
    "  - Process interrupted speech as fresh input\n",
    "  - Generate appropriate response\n",
    "  - Maintain conversation fluidity\n",
    "\n",
    "#### Success Criteria\n",
    "\n",
    "A properly implemented system will:\n",
    "- Respond immediately to interruptions\n",
    "- Understand and process interrupted speech correctly\n",
    "- Maintain natural conversation flow\n",
    "- Handle topic switches seamlessly\n",
    "- Provide appropriate responses to new questionsß"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import base64\n",
    "import json\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import websockets\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "class AudioProcessor:\n",
    "    \"\"\"Handles real-time audio processing, speech detection, and interruption management.\n",
    "    This class processes incoming audio frames, detects speech activity, and manages\n",
    "    both normal conversation flow and interruptions.\"\"\"\n",
    "    \n",
    "    def __init__(self, sample_rate=24000):\n",
    "        # Audio configuration parameters\n",
    "        self.sample_rate = sample_rate\n",
    "        \n",
    "        # Speech detection thresholds\n",
    "        # Normal speech needs to be above this level to be detected\n",
    "        self.vad_threshold = 0.015\n",
    "        # Interruptions need to exceed this higher threshold\n",
    "        self.interrupt_threshold = 0.02\n",
    "        \n",
    "        # Duration settings (in audio frames)\n",
    "        # We need at least this many frames of speech to consider it valid\n",
    "        self.min_speech_duration = int(0.3 * sample_rate)\n",
    "        # We allow this many frames of silence before ending speech detection\n",
    "        self.max_silence_duration = int(0.8 * sample_rate)\n",
    "        \n",
    "        # Frame counters for speech detection\n",
    "        self.speech_frames = 0\n",
    "        self.silence_frames = 0\n",
    "        \n",
    "        # Audio buffers\n",
    "        self.main_buffer = []         # For regular speech\n",
    "        self.interrupt_buffer = []    # For speech during interruptions\n",
    "        \n",
    "        # State tracking\n",
    "        self.is_speaking = False      # Is the AI currently speaking?\n",
    "        self.speech_detected = False  # Have we detected user speech?\n",
    "        self.is_interrupting = False  # Is an interruption in progress?\n",
    "        self.latest_audio = None      # Most recent audio frame\n",
    "\n",
    "    def process_audio(self, indata):\n",
    "        \"\"\"Process incoming audio frames for both speech detection and interruption handling.\"\"\"\n",
    "        # Keep track of the most recent audio for interruption detection\n",
    "        self.latest_audio = indata\n",
    "        audio_level = np.abs(indata).mean() / 32768.0  # Normalize to 0-1 range\n",
    "        \n",
    "        # If we're in interruption mode, store audio in interrupt buffer\n",
    "        if self.is_interrupting:\n",
    "            self.interrupt_buffer.extend(indata.tobytes())\n",
    "            return\n",
    "            \n",
    "        # Don't process for speech detection if AI is speaking\n",
    "        if self.is_speaking:\n",
    "            return\n",
    "\n",
    "        # Speech detection logic\n",
    "        if audio_level > self.vad_threshold:\n",
    "            # Speech detected\n",
    "            self.speech_detected = True\n",
    "            self.speech_frames += len(indata)\n",
    "            self.silence_frames = 0\n",
    "            self.main_buffer.extend(indata.tobytes())\n",
    "        elif self.speech_detected:\n",
    "            # Track silence after speech\n",
    "            self.silence_frames += len(indata)\n",
    "            if self.silence_frames < self.max_silence_duration:\n",
    "                self.main_buffer.extend(indata.tobytes())\n",
    "\n",
    "    def check_interruption(self):\n",
    "        \"\"\"Check if current audio level indicates an interruption while AI is speaking.\"\"\"\n",
    "        if not self.is_speaking or self.latest_audio is None:\n",
    "            return False\n",
    "            \n",
    "        audio_level = np.abs(self.latest_audio).mean() / 32768.0\n",
    "        if audio_level > self.interrupt_threshold:\n",
    "            self.is_interrupting = True\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def should_process(self):\n",
    "        \"\"\"Determine if we have enough valid speech to process.\"\"\"\n",
    "        return (self.speech_detected and \n",
    "                self.speech_frames >= self.min_speech_duration and \n",
    "                self.silence_frames >= self.max_silence_duration)\n",
    "\n",
    "    def get_main_audio(self):\n",
    "        \"\"\"Get and clear the main speech buffer.\"\"\"\n",
    "        self.speech_frames = 0\n",
    "        self.silence_frames = 0\n",
    "        self.speech_detected = False\n",
    "        audio_data = bytes(self.main_buffer)\n",
    "        self.main_buffer.clear()\n",
    "        return audio_data\n",
    "\n",
    "    def get_interrupt_audio(self):\n",
    "        \"\"\"Get and clear the interruption buffer.\"\"\"\n",
    "        if not self.interrupt_buffer:\n",
    "            return None\n",
    "        audio_data = bytes(self.interrupt_buffer)\n",
    "        self.interrupt_buffer.clear()\n",
    "        self.is_interrupting = False\n",
    "        return audio_data\n",
    "\n",
    "class ConversationSystem:\n",
    "    \"\"\"Manages the overall conversation system, handling websocket communication,\n",
    "    audio streaming, and coordinating between speech detection and AI responses.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Load environment variables and set up API connection\n",
    "        load_dotenv()\n",
    "        self.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"AZURE_OPENAI_API_KEY not found\")\n",
    "            \n",
    "        self.url = (\n",
    "            \"wss://aoai-ep-swedencentral02.openai.azure.com/openai/realtime?\"\n",
    "            f\"api-version=2024-10-01-preview&deployment=gpt-4o-realtime-preview&\"\n",
    "            f\"api-key={self.api_key}\"\n",
    "        )\n",
    "        \n",
    "        # Initialize components\n",
    "        self.audio_processor = AudioProcessor()\n",
    "        self.streams = {'input': None, 'output': None}\n",
    "\n",
    "    def audio_callback(self, indata, frames, time, status):\n",
    "        \"\"\"Callback function for audio input stream.\"\"\"\n",
    "        if status:\n",
    "            print(f\"Audio error: {status}\")\n",
    "            return\n",
    "        self.audio_processor.process_audio(indata)\n",
    "\n",
    "    async def setup_audio(self):\n",
    "        \"\"\"Initialize audio input and output streams.\"\"\"\n",
    "        self.streams['output'] = sd.OutputStream(\n",
    "            samplerate=24000, channels=1, dtype=np.int16)\n",
    "        self.streams['input'] = sd.InputStream(\n",
    "            samplerate=24000, channels=1, dtype=np.int16,\n",
    "            callback=self.audio_callback, blocksize=4800)\n",
    "            \n",
    "        for stream in self.streams.values():\n",
    "            stream.start()\n",
    "\n",
    "    async def setup_websocket_session(self, websocket):\n",
    "        \"\"\"Configure the conversation session with the API.\"\"\"\n",
    "        session_config = {\n",
    "            \"type\": \"session.update\",\n",
    "            \"session\": {\n",
    "                \"voice\": \"alloy\",\n",
    "                \"instructions\": \"You are a helpful AI assistant. Keep responses brief.\",\n",
    "                \"modalities\": [\"audio\", \"text\"],\n",
    "                \"input_audio_format\": \"pcm16\",\n",
    "                \"output_audio_format\": \"pcm16\",\n",
    "                \"turn_detection\": {\n",
    "                    \"type\": \"server_vad\",\n",
    "                    \"threshold\": 0.3,\n",
    "                    \"prefix_padding_ms\": 150,\n",
    "                    \"silence_duration_ms\": 600\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        await websocket.send(json.dumps(session_config))\n",
    "        \n",
    "        while True:\n",
    "            response = json.loads(await websocket.recv())\n",
    "            if response[\"type\"] == \"session.created\":\n",
    "                break\n",
    "            if response[\"type\"] == \"error\":\n",
    "                raise Exception(f\"Session setup failed: {response}\")\n",
    "\n",
    "    async def send_audio(self, websocket, audio_data):\n",
    "        \"\"\"Send audio data to the API.\"\"\"\n",
    "        audio_base64 = base64.b64encode(audio_data).decode('utf-8')\n",
    "        \n",
    "        # Send audio data\n",
    "        await websocket.send(json.dumps({\n",
    "            \"type\": \"input_audio_buffer.append\",\n",
    "            \"audio\": audio_base64\n",
    "        }))\n",
    "        \n",
    "        # Mark audio input as complete\n",
    "        await websocket.send(json.dumps({\n",
    "            \"type\": \"input_audio_buffer.commit\"\n",
    "        }))\n",
    "        \n",
    "        # Request response\n",
    "        await websocket.send(json.dumps({\n",
    "            \"type\": \"response.create\",\n",
    "            \"response\": {\"modalities\": [\"audio\", \"text\"]}\n",
    "        }))\n",
    "\n",
    "    async def handle_response(self, websocket):\n",
    "        \"\"\"Process AI responses and handle interruptions.\"\"\"\n",
    "        self.audio_processor.is_speaking = True\n",
    "        \n",
    "        try:\n",
    "            while True:\n",
    "                # Check for interruption\n",
    "                if self.audio_processor.check_interruption():\n",
    "                    print(\"Interrupted!\")\n",
    "                    # Cancel current response\n",
    "                    await websocket.send(json.dumps({\"type\": \"response.cancel\"}))\n",
    "                    # Wait briefly for any final speech\n",
    "                    await asyncio.sleep(0.2)\n",
    "                    # Process interrupted speech if any\n",
    "                    interrupt_audio = self.audio_processor.get_interrupt_audio()\n",
    "                    if interrupt_audio:\n",
    "                        await self.send_audio(websocket, interrupt_audio)\n",
    "                    break\n",
    "                \n",
    "                # Process normal response\n",
    "                response = json.loads(await websocket.recv())\n",
    "                \n",
    "                if response[\"type\"] == \"response.audio.delta\":\n",
    "                    if \"delta\" in response:\n",
    "                        try:\n",
    "                            # Process audio chunk\n",
    "                            audio_data = response[\"delta\"].strip()\n",
    "                            padding = -len(audio_data) % 4\n",
    "                            if padding:\n",
    "                                audio_data += \"=\" * padding\n",
    "                            \n",
    "                            audio = np.frombuffer(\n",
    "                                base64.b64decode(audio_data), \n",
    "                                dtype=np.int16\n",
    "                            )\n",
    "                            self.streams['output'].write(audio)\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"Audio processing error: {e}\")\n",
    "                            \n",
    "                elif response[\"type\"] == \"response.done\":\n",
    "                    break\n",
    "                    \n",
    "        finally:\n",
    "            self.audio_processor.is_speaking = False\n",
    "\n",
    "    async def run(self):\n",
    "        \"\"\"Main conversation loop.\"\"\"\n",
    "        await self.setup_audio()\n",
    "        print(\"Audio setup complete\")\n",
    "        \n",
    "        async with websockets.connect(self.url) as ws:\n",
    "            await self.setup_websocket_session(ws)\n",
    "            print(\"Ready for conversation\")\n",
    "            \n",
    "            while True:\n",
    "                # Process normal speech\n",
    "                if self.audio_processor.should_process():\n",
    "                    audio_data = self.audio_processor.get_main_audio()\n",
    "                    await self.send_audio(ws, audio_data)\n",
    "                    await self.handle_response(ws)\n",
    "                    \n",
    "                await asyncio.sleep(0.05)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    system = ConversationSystem()\n",
    "    asyncio.run(system.run())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following [script](https://github.com/ozgurgulerx/ai-builders/blob/main/part1_build/p1uc1_realtime_api_converse_step3_interruption.py) to run the real-time conversation system..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 4 **Session and Context Management"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
