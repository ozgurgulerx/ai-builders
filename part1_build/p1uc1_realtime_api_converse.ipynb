{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONVERSE with REALTIME API \n",
    "In this step, we'll transform our single-turn audio generation into a full conversational system. This involves tackling several key challenges: managing bi-directional audio streams for both user input and AI output, implementing proper turn-taking mechanics (knowing when the user has finished speaking and when the AI should respond), handling the complex flow of events from speech detection to response generation, and maintaining conversation context across multiple exchanges. The trickiest part isn't just streaming the audio - it's orchestrating all these components to work together smoothly while maintaining natural conversational flow. We'll build this step by step, focusing on creating a robust system that can handle real-world conversation scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Challenges in Building Real-time Conversational AI\n",
    "\n",
    "Below are the key challanges in building Real-time Conversational AI...\n",
    "\n",
    "1. **Session and Context Management**\n",
    "  - Maintaining conversation history\n",
    "  - Tracking context across exchanges\n",
    "  - Managing session state\n",
    "\n",
    "2. **Turn-Taking Mechanics**\n",
    "  - Detecting end of user speech\n",
    "  - Managing interruptions\n",
    "  - Coordinating input/output transitions\n",
    "  - Timing audio recording and playback\n",
    "\n",
    "3. **Audio Stream Handling**\n",
    "  - Coordinating bidirectional audio\n",
    "  - Managing stream lifecycles\n",
    "  - Resolving device conflicts\n",
    "  - Handling format compatibility\n",
    "\n",
    "4. **Event Flow Control**\n",
    "  - Input audio streaming\n",
    "  - Speech detection\n",
    "  - Text transcription\n",
    "  - Response generation\n",
    "  - Output audio streaming\n",
    "\n",
    "5. **Error Recovery**\n",
    "  - Connection drops\n",
    "  - Audio device issues\n",
    "  - API rate limits\n",
    "  - Graceful error handling\n",
    "\n",
    "6. **Resource Management**\n",
    "  - WebSocket connection handling\n",
    "  - Audio buffer memory\n",
    "  - Resource cleanup\n",
    "  - Connection lifecycle management\n",
    "\n",
    "\n",
    "We will build an MVP, therefore choose the minimum set that will give us a working setup. The below is what we will target. \n",
    "# Minimum Viable Conversational System - Development Plan\n",
    "\n",
    "## Phase 1: Basic Turn-Taking\n",
    "1. **Core Components**\n",
    "  - Audio input/output stream management\n",
    "  - Basic turn detection\n",
    "  - Session context tracking\n",
    "\n",
    "## Implementation Steps\n",
    "\n",
    "### Step 1: Input/Output Setup\n",
    "- Configure audio device for both input and output\n",
    "- Implement basic audio streaming\n",
    "\n",
    "### Step 2: Turn Detection\n",
    "- Add VAD (Voice Activity Detection)\n",
    "- Implement simple silence detection\n",
    "- Basic interrupt handling\n",
    "\n",
    "### Step 3: Context Management\n",
    "- Track conversation history\n",
    "- Maintain basic session state\n",
    "\n",
    "## Testing Milestones\n",
    "1. Test audio I/O\n",
    "2. Verify turn detection\n",
    "3. Validate context preservation\n",
    "\n",
    "Timeline: We will implement each step iteratively, testing thoroughly before moving to next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 **Session and Context Management**\n",
    "We will implement the below key features implemented:\n",
    "\n",
    "- Bidirectional audio streams\n",
    "- Basic turn management\n",
    "- Audio buffering\n",
    "- Session handling\n",
    "- Basic error recovery\n",
    "\n",
    "Finally test by running the script - it will listen for 2 seconds, send to API, then play response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import base64\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import websockets\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "\n",
    "class ConversationSystem:\n",
    "    def __init__(self):\n",
    "        load_dotenv()\n",
    "        self.input_stream = None\n",
    "        self.output_stream = None\n",
    "        self.is_speaking = False\n",
    "        self.input_buffer = []\n",
    "        self.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"AZURE_OPENAI_API_KEY not found in environment\")\n",
    "        self.url = (\n",
    "            f\"wss://aoai-ep-swedencentral02.openai.azure.com/openai/realtime?\"\n",
    "            f\"api-version=2024-10-01-preview&deployment=gpt-4o-realtime-preview&api-key={self.api_key}\"\n",
    "        )\n",
    "\n",
    "    async def setup_audio(self):\n",
    "        print(\"Setting up audio streams...\")\n",
    "        self.output_stream = sd.OutputStream(samplerate=24000, channels=1, dtype=np.int16)\n",
    "        self.input_stream = sd.InputStream(samplerate=24000, channels=1, dtype=np.int16,\n",
    "                                         callback=self.audio_callback)\n",
    "        self.output_stream.start()\n",
    "        self.input_stream.start()\n",
    "        print(\"Audio streams started\")\n",
    "        \n",
    "    def audio_callback(self, indata, frames, time, status):\n",
    "        if status:\n",
    "            print(f\"Input stream error: {status}\")\n",
    "        if not self.is_speaking:\n",
    "            self.input_buffer.extend(indata.tobytes())\n",
    "\n",
    "    async def start_conversation(self):\n",
    "        try:\n",
    "            async with websockets.connect(self.url) as ws:\n",
    "                print(\"Connected to WebSocket\")\n",
    "                await self.setup_session(ws)\n",
    "                \n",
    "                # Initial greeting\n",
    "                await self.send_message(ws, \"Hello\")\n",
    "                await self.handle_response(ws)\n",
    "\n",
    "                while True:\n",
    "                    print(\"\\nListening... (speak for at least 2 seconds)\")\n",
    "                    self.input_buffer.clear()\n",
    "                    await asyncio.sleep(2)  # Wait for 2 seconds of audio\n",
    "\n",
    "                    if len(self.input_buffer) > 0:\n",
    "                        print(\"Processing your input...\")\n",
    "                        audio_data = bytes(self.input_buffer)\n",
    "                        self.input_buffer.clear()\n",
    "                        \n",
    "                        # Send audio\n",
    "                        base64_audio = base64.b64encode(audio_data).decode('utf-8')\n",
    "                        await ws.send(json.dumps({\n",
    "                            \"type\": \"input_audio_buffer.append\",\n",
    "                            \"audio\": base64_audio\n",
    "                        }))\n",
    "                        await ws.send(json.dumps({\n",
    "                            \"type\": \"input_audio_buffer.commit\"\n",
    "                        }))\n",
    "                        \n",
    "                        # Request response\n",
    "                        await ws.send(json.dumps({\n",
    "                            \"type\": \"response.create\",\n",
    "                            \"response\": {\"modalities\": [\"audio\", \"text\"]}\n",
    "                        }))\n",
    "                        \n",
    "                        await self.handle_response(ws)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in conversation: {e}\")\n",
    "\n",
    "    async def setup_session(self, ws):\n",
    "        session_payload = {\n",
    "            \"type\": \"session.update\",\n",
    "            \"session\": {\n",
    "                \"voice\": \"alloy\",\n",
    "                \"instructions\": \"You are a helpful AI assistant. Keep responses brief and engaging.\",\n",
    "                \"modalities\": [\"audio\", \"text\"],\n",
    "                \"input_audio_format\": \"pcm16\",\n",
    "                \"output_audio_format\": \"pcm16\",\n",
    "                \"turn_detection\": {\n",
    "                    \"type\": \"server_vad\",\n",
    "                    \"threshold\": 0.5,\n",
    "                    \"prefix_padding_ms\": 300,\n",
    "                    \"silence_duration_ms\": 200\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        await ws.send(json.dumps(session_payload))\n",
    "        \n",
    "        while True:\n",
    "            response = await ws.recv()\n",
    "            data = json.loads(response)\n",
    "            if data.get(\"type\") == \"session.created\":\n",
    "                print(\"Session setup complete\")\n",
    "                break\n",
    "            elif data.get(\"type\") == \"error\":\n",
    "                raise Exception(\"Error creating session\")\n",
    "\n",
    "    async def send_message(self, ws, text):\n",
    "        message_payload = {\n",
    "            \"type\": \"conversation.item.create\",\n",
    "            \"item\": {\n",
    "                \"type\": \"message\",\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"type\": \"input_text\", \"text\": text}]\n",
    "            }\n",
    "        }\n",
    "        await ws.send(json.dumps(message_payload))\n",
    "        \n",
    "        await ws.send(json.dumps({\n",
    "            \"type\": \"response.create\",\n",
    "            \"response\": {\"modalities\": [\"audio\", \"text\"]}\n",
    "        }))\n",
    "\n",
    "    async def handle_response(self, ws):\n",
    "        self.is_speaking = True\n",
    "        try:\n",
    "            while True:\n",
    "                response = await ws.recv()\n",
    "                data = json.loads(response)\n",
    "                \n",
    "                if data[\"type\"] == \"response.audio.delta\":\n",
    "                    if \"delta\" in data:\n",
    "                        try:\n",
    "                            audio_data = data[\"delta\"].replace(\" \", \"\").replace(\"\\n\", \"\")\n",
    "                            padding = len(audio_data) % 4\n",
    "                            if padding:\n",
    "                                audio_data += \"=\" * padding\n",
    "                            \n",
    "                            audio_bytes = base64.b64decode(audio_data)\n",
    "                            audio = np.frombuffer(audio_bytes, dtype=np.int16)\n",
    "                            self.output_stream.write(audio)\n",
    "                            print(\".\", end=\"\", flush=True)\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing audio: {e}\")\n",
    "                            \n",
    "                elif data[\"type\"] == \"response.done\":\n",
    "                    break\n",
    "                    \n",
    "        finally:\n",
    "            self.is_speaking = False\n",
    "\n",
    "async def main():\n",
    "    system = ConversationSystem()\n",
    "    await system.setup_audio()\n",
    "    await system.start_conversation()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting real-time conversation system...\")\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the above as a python script outside of the notebook as event loops clash with Jupyters scheduling. \\\n",
    "[Here is a working script](https://github.com/ozgurgulerx/ai-builders/blob/main/part1_build/p1uc1_realtime_api_converse_step1.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila - our voice chatbot works!!! \\\n",
    "Still a bit clumsy though in terms of detecting session dynamics as well as properly listenning and understanding the user. \\\n",
    "We will improve it in the next steps.\n",
    "\n",
    "Implementation Status: Phase 1 \\\n",
    "\n",
    "#### âœ“ Completed\n",
    "- Audio I/O setup (device configuration, streaming)\n",
    "- Server-side VAD implementation\n",
    "- Basic audio state management\n",
    "\n",
    "#### ðŸ”„ Partially Implemented\n",
    "- Turn detection (needs refinement)\n",
    "- Basic interrupt handling\n",
    "- State tracking\n",
    "\n",
    "##### âŒ Not Implemented\n",
    "- Conversation history\n",
    "- Context preservation\n",
    "- Session state management\n",
    "\n",
    "#### Next Steps\n",
    "Focus on context management system and turn detection improvements for Phase 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP2 - Turn Detection, better VAD and Silence Detection \n",
    "\n",
    "#### Voice Activity Detection (VAD)\n",
    "- Configurable VAD threshold\n",
    "- Proper silence duration tracking\n",
    "- Minimum speech frames validation\n",
    "\n",
    "#### Audio Processing\n",
    "- Block-based processing (200ms blocks)\n",
    "- Efficient buffer management\n",
    "- Real-time audio level monitoring\n",
    "\n",
    "#### State Management\n",
    "- Clear speech state tracking\n",
    "- Better interruption handling\n",
    "- Dynamic silence detection\n",
    "\n",
    "Expected outcome: More natural conversation flow with reliable speech detection and turn-taking.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import base64\n",
    "import json\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import websockets\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "class AudioProcessor:\n",
    "    def __init__(self, sample_rate=24000):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.vad_threshold = 0.015\n",
    "        self.speech_frames = 0\n",
    "        self.silence_frames = 0\n",
    "        self.min_speech_duration = int(0.3 * sample_rate)\n",
    "        self.max_silence_duration = int(0.8 * sample_rate)\n",
    "        self.buffer = []\n",
    "        self.is_speaking = False\n",
    "        self.speech_detected = False\n",
    "\n",
    "    def process_audio(self, indata):\n",
    "        if self.is_speaking:\n",
    "            return\n",
    "\n",
    "        audio_level = np.abs(indata).mean() / 32768.0\n",
    "        \n",
    "        if audio_level > self.vad_threshold:\n",
    "            self.speech_detected = True\n",
    "            self.speech_frames += len(indata)\n",
    "            self.silence_frames = 0\n",
    "            self.buffer.extend(indata.tobytes())\n",
    "        elif self.speech_detected:\n",
    "            self.silence_frames += len(indata)\n",
    "            if self.silence_frames < self.max_silence_duration:\n",
    "                self.buffer.extend(indata.tobytes())\n",
    "\n",
    "    def should_process(self):\n",
    "        return (self.speech_detected and \n",
    "                self.speech_frames >= self.min_speech_duration and \n",
    "                self.silence_frames >= self.max_silence_duration)\n",
    "\n",
    "    def reset(self):\n",
    "        self.speech_frames = 0\n",
    "        self.silence_frames = 0\n",
    "        self.speech_detected = False\n",
    "        audio_data = bytes(self.buffer)\n",
    "        self.buffer.clear()\n",
    "        return audio_data\n",
    "\n",
    "class ConversationSystem:\n",
    "    def __init__(self):\n",
    "        load_dotenv()\n",
    "        self.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"AZURE_OPENAI_API_KEY not found\")\n",
    "            \n",
    "        self.url = (\n",
    "            \"wss://aoai-ep-swedencentral02.openai.azure.com/openai/realtime?\"\n",
    "            f\"api-version=2024-10-01-preview&deployment=gpt-4o-realtime-preview&\"\n",
    "            f\"api-key={self.api_key}\"\n",
    "        )\n",
    "        \n",
    "        self.audio_processor = AudioProcessor()\n",
    "        self.streams = {'input': None, 'output': None}\n",
    "\n",
    "    def audio_callback(self, indata, frames, time, status):\n",
    "        if status:\n",
    "            print(f\"Audio error: {status}\")\n",
    "            return\n",
    "        self.audio_processor.process_audio(indata)\n",
    "\n",
    "    async def setup_audio(self):\n",
    "        self.streams['output'] = sd.OutputStream(\n",
    "            samplerate=24000, channels=1, dtype=np.int16)\n",
    "        self.streams['input'] = sd.InputStream(\n",
    "            samplerate=24000, channels=1, dtype=np.int16,\n",
    "            callback=self.audio_callback, blocksize=4800)\n",
    "            \n",
    "        for stream in self.streams.values():\n",
    "            stream.start()\n",
    "\n",
    "    async def setup_websocket_session(self, websocket):\n",
    "        session_config = {\n",
    "            \"type\": \"session.update\",\n",
    "            \"session\": {\n",
    "                \"voice\": \"alloy\",\n",
    "                \"instructions\": \"You are a helpful AI assistant. Keep responses brief.\",\n",
    "                \"modalities\": [\"audio\", \"text\"],\n",
    "                \"input_audio_format\": \"pcm16\",\n",
    "                \"output_audio_format\": \"pcm16\",\n",
    "                \"turn_detection\": {\n",
    "                    \"type\": \"server_vad\",\n",
    "                    \"threshold\": 0.3,\n",
    "                    \"prefix_padding_ms\": 150,\n",
    "                    \"silence_duration_ms\": 600\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        await websocket.send(json.dumps(session_config))\n",
    "        \n",
    "        while True:\n",
    "            response = json.loads(await websocket.recv())\n",
    "            if response[\"type\"] == \"session.created\":\n",
    "                break\n",
    "            if response[\"type\"] == \"error\":\n",
    "                raise Exception(f\"Session setup failed: {response}\")\n",
    "\n",
    "    async def send_audio(self, websocket, audio_data):\n",
    "        audio_base64 = base64.b64encode(audio_data).decode('utf-8')\n",
    "        \n",
    "        await websocket.send(json.dumps({\n",
    "            \"type\": \"input_audio_buffer.append\",\n",
    "            \"audio\": audio_base64\n",
    "        }))\n",
    "        await websocket.send(json.dumps({\"type\": \"input_audio_buffer.commit\"}))\n",
    "        await websocket.send(json.dumps({\n",
    "            \"type\": \"response.create\",\n",
    "            \"response\": {\"modalities\": [\"audio\", \"text\"]}\n",
    "        }))\n",
    "\n",
    "    async def handle_response(self, websocket):\n",
    "        self.audio_processor.is_speaking = True\n",
    "        \n",
    "        try:\n",
    "            while True:\n",
    "                response = json.loads(await websocket.recv())\n",
    "                \n",
    "                if response[\"type\"] == \"response.audio.delta\":\n",
    "                    if \"delta\" in response:\n",
    "                        try:\n",
    "                            audio_data = response[\"delta\"].strip()\n",
    "                            padding = -len(audio_data) % 4\n",
    "                            if padding:\n",
    "                                audio_data += \"=\" * padding\n",
    "                            \n",
    "                            audio = np.frombuffer(\n",
    "                                base64.b64decode(audio_data), \n",
    "                                dtype=np.int16\n",
    "                            )\n",
    "                            self.streams['output'].write(audio)\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"Audio processing error: {e}\")\n",
    "                            \n",
    "                elif response[\"type\"] == \"response.done\":\n",
    "                    break\n",
    "                    \n",
    "        finally:\n",
    "            self.audio_processor.is_speaking = False\n",
    "\n",
    "    async def run(self):\n",
    "        await self.setup_audio()\n",
    "        \n",
    "        async with websockets.connect(self.url) as ws:\n",
    "            await self.setup_websocket_session(ws)\n",
    "            print(\"Ready for conversation\")\n",
    "            \n",
    "            while True:\n",
    "                if self.audio_processor.should_process():\n",
    "                    audio_data = self.audio_processor.reset()\n",
    "                    await self.send_audio(ws, audio_data)\n",
    "                    await self.handle_response(ws)\n",
    "                await asyncio.sleep(0.05)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    system = ConversationSystem()\n",
    "    asyncio.run(system.run())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adjust the VAD parameters if the conversation is not fluid in your system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Here is a working script](https://github.com/ozgurgulerx/ai-builders/blob/main/part1_build/p1uc1_realtime_api_converse_step2.py)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
